from typing import Dict, Any, List, Optional, Callable
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage
from langchain.prompts import PromptTemplate
from langchain_core.documents import Document
from langgraph.graph import StateGraph, MessagesState, START, END
from langgraph.checkpoint.memory import MemorySaver
from langgraph.graph.state import CompiledStateGraph
from pydantic import BaseModel, Field
import operator
from typing_extensions import Annotated

from core.llm_setup import setup_llm, setup_retriever
from core.langgraph_memory import LangGraphMemoryManager
from config.prompts import get_qa_prompt
from utils.simple_transparency import (
    update_processing_status,
    complete_processing_status
)
from utils.transparency_system import ProcessingStage
import re


class RAGState(BaseModel):
    """State for the RAG workflow"""
    messages: Annotated[List[BaseMessage], operator.add] = Field(default_factory=list)
    question: str = ""
    context: List[Document] = Field(default_factory=list)
    answer: str = ""
    chat_history: List[BaseMessage] = Field(default_factory=list)


def clean_response(response: str, user_question: str) -> str:
    """Clean the response to remove any repetition of the user's question"""
    question_clean = re.escape(user_question.strip())
    pattern = f"^{question_clean}[\\s\\n]*"
    cleaned = re.sub(pattern, "", response.strip(), flags=re.IGNORECASE | re.MULTILINE)
    
    prefixes_to_remove = [
        r"^.*question.*:.*\n*",
        r"^.*demande.*:.*\n*",
        r"^.*vous demandez.*:.*\n*",
        r"^.*concernant votre question.*:.*\n*"
    ]
    
    for prefix in prefixes_to_remove:
        cleaned = re.sub(prefix, "", cleaned, flags=re.IGNORECASE | re.MULTILINE)
    
    return cleaned.strip()


class LangGraphRAGChain:
    """
    LangGraph-based RAG chain that replaces ConversationalRetrievalChain
    with enhanced memory management and workflow control
    """
    
    def __init__(self, memory_manager: LangGraphMemoryManager, collection_key: str = None, 
                 prompt_name: str = "caracterologie_qa", prompt_version: int = None):
        """
        Initialize LangGraph RAG chain
        
        Args:
            memory_manager: LangGraph memory manager instance
            collection_key: Vector store collection key
            prompt_name: Prompt template name
            prompt_version: Prompt version
        """
        self.memory_manager = memory_manager
        self.llm = setup_llm()
        self.retriever = setup_retriever(collection_key)
        self.prompt_name = prompt_name
        self.prompt_version = prompt_version
        
        # Build the workflow graph
        self.workflow = self._build_workflow()
        self.app = self.workflow.compile()
    
    def _build_workflow(self) -> StateGraph:
        """Build the LangGraph workflow for RAG"""
        workflow = StateGraph(RAGState)
        
        # Add nodes
        workflow.add_node("retrieve_context", self._retrieve_context)
        workflow.add_node("contextualize_question", self._contextualize_question)
        workflow.add_node("generate_answer", self._generate_answer)
        
        # Add edges
        workflow.add_edge(START, "retrieve_context")
        workflow.add_edge("retrieve_context", "contextualize_question")
        workflow.add_edge("contextualize_question", "generate_answer")
        workflow.add_edge("generate_answer", END)
        
        return workflow
    
    def _retrieve_context(self, state: RAGState) -> Dict[str, Any]:
        """Retrieve relevant documents based on the question"""
        # Update transparency: starting document retrieval
        update_processing_status(ProcessingStage.DOCUMENT_RETRIEVAL)
        
        print(f"\nüîç RECHERCHE DE CHUNKS pour la question: '{state.question}'")
        print("=" * 80)
        
        # Use the original question for retrieval (before contextualization)
        docs = self.retriever.invoke(state.question)
        
        
        print(f"üìÑ {len(docs)} chunks r√©cup√©r√©s:")
        print("-" * 80)
        for i, doc in enumerate(docs, 1):
            print(f"\nüìÑ Chunk {i}:")
            if hasattr(doc, 'metadata'):
                print(f"   Source: {doc.metadata.get('source', 'N/A')}")
                print(f"   Page: {doc.metadata.get('page', 'N/A')}")
            content = doc.page_content if hasattr(doc, 'page_content') else str(doc)
            print(f"   Contenu: {content[:200]}...")
            if len(content) > 200:
                print(f"   (tronqu√©, longueur totale: {len(content)} caract√®res)")
            print("-" * 40)
        print("=" * 80)
        
        return {"context": docs}
    
    def _contextualize_question(self, state: RAGState) -> Dict[str, Any]:
        """Contextualize the question using chat history if needed"""
        # Update transparency: starting context generation
        update_processing_status(ProcessingStage.CONTEXT_GENERATION)
        
        # If no chat history, return question as-is
        if not state.chat_history:
            return {"question": state.question}
        
        # Create contextualization prompt
        contextualize_prompt = """√âtant donn√© un historique de conversation et la derni√®re question de l'utilisateur qui pourrait faire r√©f√©rence au contexte de l'historique de conversation, formulez une question autonome qui peut √™tre comprise sans l'historique de conversation. Ne r√©pondez PAS √† la question, reformulez-la uniquement si n√©cessaire, sinon retournez-la telle quelle.

Historique de conversation:
{chat_history}

Question: {question}

Question contextualis√©e:"""
        
        # Format chat history
        history_text = ""
        for msg in state.chat_history[-6:]:  # Last 6 messages for context
            role = "Utilisateur" if isinstance(msg, HumanMessage) else "Assistant"
            history_text += f"{role}: {msg.content}\n"
        
        prompt = contextualize_prompt.format(
            chat_history=history_text,
            question=state.question
        )
        
        # Get contextualized question
        contextualized_question = self.llm.invoke(prompt).content
        
        return {"question": contextualized_question}
    
    def _generate_answer(self, state: RAGState) -> Dict[str, Any]:
        """Generate answer using retrieved context and chat history"""
        # Update transparency: starting response generation
        update_processing_status(ProcessingStage.RESPONSE_GENERATION)
        
        # Display memory content
        if state.chat_history:
            print(f"\nüí¨ M√âMOIRE DE CONVERSATION ({len(state.chat_history)} messages):")
            print("=" * 80)
            for i, msg in enumerate(state.chat_history, 1):
                role = "üë§ Utilisateur" if isinstance(msg, HumanMessage) else "ü§ñ Assistant"
                print(f"\n{i}. {role}:")
                print("-" * 40)
                print(msg.content)
                print("-" * 40)
                print(f"üìä Longueur: {len(msg.content)} caract√®res")
            print("=" * 80)
        else:
            print("\nüí¨ M√âMOIRE DE CONVERSATION: (vide)")
        
        # Create enhanced prompt template
        enhanced_template = """Tu es un assistant caract√©rologue expert, √† la fois p√©dagogue et curieux. Ton r√¥le est de faire d√©couvrir la caract√©rologie ‚Äî la science des types de caract√®re ‚Äî de mani√®re √† la fois pr√©cise, vivante et accessible.

Tu r√©ponds aux questions des utilisateurs en t'appuyant rigoureusement sur les connaissances fournies par la base de donn√©es int√©gr√©e, notamment les travaux de Ren√© Le Senne et les typologies reconnues (√©motivit√©, activit√©, retentissement). Si une r√©ponse n'est pas disponible dans les sources, tu l'indiques honn√™tement.

Tu adaptes ton langage et ton niveau d'explication selon le profil de l'utilisateur (novice ou initi√©). Tu cherches √† l'accompagner dans sa compr√©hension de la caract√©rologie. S'il pose des questions simples ou g√©n√©rales, tu proposes des compl√©ments pertinents pour approfondir.

Tu es capable d'orienter la conversation de fa√ßon naturelle, en sugg√©rant des sujets li√©s √† ce que l'utilisateur vient de dire. Par exemple, tu peux l'inviter √† d√©couvrir un autre type psychologique, une dimension caract√©rologique ou une mise en application concr√®te.

Tu peux aussi poser des questions ouvertes √† l'utilisateur s'il semble curieux mais ne sait pas par o√π commencer.

Sois clair, structur√© et rigoureux. Utilise des exemples concrets si cela peut aider √† mieux comprendre. Ton objectif : √©veiller l'int√©r√™t, transmettre un savoir solide, et guider pas √† pas dans l'univers de la caract√©rologie.

Adapte la longueur de ta r√©ponse √† la complexit√© de la question : r√©ponse courte pour une question simple, plus d√©velopp√©e pour une question complexe ou une demande d'explication d√©taill√©e.

IMPORTANT - Utilise les informations suivantes dans cet ordre de priorit√© :

1. HISTORIQUE DE CONVERSATION (priorit√© absolue pour comprendre les r√©f√©rences comme "√ßa", "ils", "cette notion") :
{chat_history}

2. CONTEXTE DOCUMENTAIRE (sources pour informations factuelles) :
{context}

3. QUESTION ACTUELLE :
{input}"""
        
        # Format chat history for prompt
        history_text = ""
        if state.chat_history:
            for msg in state.chat_history:
                role = "Utilisateur" if isinstance(msg, HumanMessage) else "Assistant"
                history_text += f"{role}: {msg.content}\n"
        else:
            history_text = "(Aucun historique)"
        
        # Format context
        context_text = ""
        for doc in state.context:
            context_text += doc.page_content + "\n\n"
        
        # Create final prompt
        final_prompt = enhanced_template.format(
            chat_history=history_text,
            context=context_text,
            input=state.question
        )
        
        print(f"\nü§ñ PROMPT SYST√àME UTILIS√â:")
        print("=" * 80)
        print("‚úÖ Prompt avec historique de conversation int√©gr√©")
        print("‚úÖ Priorit√© donn√©e √† l'historique pour les r√©f√©rences")
        print("=" * 80)
        
        # Generate response
        response = self.llm.invoke(final_prompt)
        answer = response.content
        
        return {"answer": answer}
    
    def invoke(self, inputs: Dict[str, Any], config: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """
        Invoke the RAG chain with memory management
        
        Args:
            inputs: Input dictionary containing "question"
            config: Optional configuration for callbacks
            
        Returns:
            Dictionary containing "answer"
        """
        question = inputs["question"]
        thread_id = self.memory_manager.get_current_thread()
        
        # Get chat history from memory manager
        chat_history = self.memory_manager.get_chat_history()
        
        # Prepare initial state
        initial_state = RAGState(
            question=question,
            chat_history=chat_history
        )
        
        # Handle streaming if specified in config
        stream_handler = None
        if config and "callbacks" in config:
            for cb in config["callbacks"]:
                if hasattr(cb, '__class__') and 'StreamlitCallbackHandler' in cb.__class__.__name__:
                    stream_handler = cb
                    break
        
        # Run the workflow
        final_state = self.app.invoke(initial_state)
        
        answer = final_state["answer"]
        
        # Handle streaming display
        if stream_handler:
            import time
            words = answer.split()
            displayed_text = ""
            
            for i, word in enumerate(words):
                displayed_text += word + " "
                if i % 3 == 0:  # Update every 3 words
                    try:
                        stream_handler.placeholder.markdown(displayed_text + "‚ñå")
                        time.sleep(0.05)
                    except:
                        pass
            
            # Final update without cursor
            try:
                stream_handler.placeholder.markdown(displayed_text.strip())
            except:
                pass
        
        # Save context to memory
        update_processing_status(ProcessingStage.MEMORY_SAVING)
        self.memory_manager.save_context(
            {"question": question},
            {"answer": answer}
        )
        
        # Complete transparency tracking
        complete_processing_status()
        
        return {"answer": answer}


def setup_langgraph_qa_chain(memory_manager: LangGraphMemoryManager, collection_key: str = None, 
                            prompt_name: str = "caracterologie_qa", prompt_version: int = None) -> LangGraphRAGChain:
    """
    Set up a LangGraph-based RAG chain with memory management
    
    Args:
        memory_manager: LangGraph memory manager instance
        collection_key: Vector store collection key
        prompt_name: Prompt template name
        prompt_version: Prompt version
        
    Returns:
        LangGraphRAGChain instance
    """
    return LangGraphRAGChain(memory_manager, collection_key, prompt_name, prompt_version)


# Backward compatibility function
def setup_qa_chain_with_memory(memory, collection_key: str = None, prompt_name: str = "caracterologie_qa", prompt_version: int = None):
    """
    Backward compatibility function that works with both old and new memory systems
    """
    # Check if it's the new LangGraph memory manager
    if hasattr(memory, 'manager') and hasattr(memory.manager, '_is_langgraph_memory'):
        return setup_langgraph_qa_chain(memory.manager, collection_key, prompt_name, prompt_version)
    
    # Fall back to old implementation if needed
    from core.qa_chain import setup_qa_chain_with_memory as old_setup
    return old_setup(memory, collection_key, prompt_name, prompt_version)